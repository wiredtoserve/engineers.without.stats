count_n = 0
count_skn = 0
count_t = 0
count_skt = 0
count_neg = 0
for(i in 1:nrow(probs)){
if(gdp_tst$INFL[i] < 0){
count_neg = count_neg + 1
#normal
if(probs[i, 1] > threshold){
count_n = count_n + 1
}
# skew normal
if(probs[i, 2] > threshold){
count_skn = count_skn + 1
}
# t
if(probs[i, 3] > threshold){
count_t = count_t + 1
}
# skew t
if(probs[i, 4] > threshold){
count_skt = count_skt + 1
}
}
}
count_n
count_skn
count_t
count_skt
count_neg
par(mfrow=c(1,2))
plot(probs[,1],type="l",ylim=c(0,max(probs)),main="Prob of negative inflation", ylab="Probability")
lines(probs[,2],type="l",col="red")
lines(probs[,3],type="l",col="blue")
lines(probs[,4],type="l",col="green")
plot(gdp_tst$GDP,type="l", main="Actual inflation over the test set", ylab="Inflation")
# Question 17
fcast_accuracy(yfs_AR1,gdp_tst$INFL,gdp_tst$INFL_LAG)
fcast_accuracy(yfs_lag,gdp_tst$INFL,gdp_tst$INFL_LAG)
# set up expanding window forecast
tmp_trn <- d_auto_trn
L <- length(d_auto_tst)
yfs_auto <- numeric(L)*NaN
yfs_a2 <- numeric(L)*NaN
for(i in 1:L){
print(i)
# fit ARIMA model to new the current training data
fit_tmp_auto <- Arima(tmp_trn, order=c(2,0,1), seasonal=c(1,0,2))
fit_tmp_a2 <- Arima(tmp_trn, order=c(2,0,2), seasonal =c(0,0,0))
# forecast 1 step ahead and save point forecast
fc_tmp_auto <- forecast(fit_tmp_auto, h=1)
fc_tmp_a2 <- forecast(fit_tmp_a2, h=1)
yfs_auto[i] <- fc_tmp_auto$mean
yfs_a2[i] <- fc_tmp_a2$mean
# make new training time series by glueing on previous test value
train_start_date = time(d_auto_trn)[1]
tmp_trn <- ts(c(tmp_trn, d_auto_tst[i]),
frequency = 12,
start = train_start_date)
}
test_start_date <- time(d_auto_tst)[1]
yfs_auto <- ts(yfs_auto, frequency=12, start=test_start_date)
yfs_a2 <- ts(yfs_a2, frequency=12, start=test_start_date)
# check accuracy
ylag <- c(d_auto_trn[length(d_auto_trn)], d_auto_tst[1:64])
fcast_accuracy(yfs_auto, d_auto_tst, ylag)
# check accuracy
ylag <- c(d_auto_trn[length(d_auto_trn)], d_auto_tst[1:64])
fcast_accuracy(yfs_auto, d_auto_tst, ylag)
fcast_accuracy(yfs_a2, d_auto_tst, ylag)
fcast_accuracy(yfs_a2, d_auto_tst, ylag)
yfs_a2
yfs_a2 - d_auto_tst
a1 <- yfs_a2 - d_auto_tst
a3 <- yfs_auto - d_auto_tst
# is HW1 different to HW3?
dm.test(a1, a3, alternative="two.sided", power=2)
# is HW3 better than HW1?
dm.test(a1, a3, alternative="greater", power=2)
a1 <- yfs_a2 - d_auto_tst
a3 <- yfs_auto - d_auto_tst
# is HW1 different to HW3?
dm.test(a1, a3, alternative="two.sided", power=2)
# is HW3 better than HW1?
dm.test(a3, a1, alternative="l", power=2)
qnorm(0.01)
mu$mu
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(moments)
library(matrixStats)
library(knitr)
library(ggplot2)
library(lubridate)
start="2004-01-01"
# Apple
pAAPL=get.hist.quote("AAPL", start=start, quote = "Adjusted",retclass="zoo",compression="m")
# Boeing
pBA=get.hist.quote("BA", start=start, quote = "Adjusted",retclass="zoo",compression="m")
# Johnson and Johnson
pJNJ=get.hist.quote("JNJ", start=start, quote = "Adjusted",retclass="zoo",compression="m")
# JP Morgan
pJPM=get.hist.quote("JPM", start=start, quote = "Adjusted",retclass="zoo",compression="m")
# First Energy
pFE=get.hist.quote("FE", start=start, quote = "Adjusted",retclass="zoo",compression="m")
# Walt Disney
pDIS=get.hist.quote("DIS", start=start, quote = "Adjusted",retclass="zoo",compression="m")
# Verizon
pVZ=get.hist.quote("VZ", start=start, quote = "Adjusted",retclass="zoo",compression="m")
# Dow Inc
pPG=get.hist.quote("PG", start=start, quote = "Adjusted",retclass="zoo",compression="m")
stocks <- cbind(pAAPL, pJNJ, pJPM, pFE, pDIS, pVZ, pPG)
#data.frame(diff(as.matrix(stocks)))
#data <- cbind(diff(pAAPL)/pAAPL[2:length(pAAPL)], diff(pBA)/pBA[2:length(pBA)], diff(pJNJ)/pJNJ[2:length(pJNJ)], diff(pJPM)/pJPM[2:length(pJPM)], diff(pFE)/pFE[2:length(pFE)], diff(pDIS)/pDIS[2:length(pDIS)], diff(pVZ)/pVZ[2:length(pVZ)], diff(pPG)/pPG[2:length(pPG)])
#data <- cbind(diff(pAAPL)/pAAPL[1:(length(pAAPL)-1)], diff(pBA)/pBA[1:(length(pBA)-1)], diff(pJNJ)/pJNJ[1:(length(pJNJ)-1)], diff(pJPM)/pJPM[1:(length(pJPM)-1)], diff(pFE)/pFE[1:(length(pFE)-1)], diff(pDIS)/pDIS[1:(length(pDIS)-1)], diff(pVZ)/pVZ[1:(length(pVZ)-1)], diff(pPG)/pPG[1:(length(pPG)-1)])
data <- read.csv('returns.csv', header = TRUE)
#stocks <- as.matrix(stocks)
head(data)
#returns_data = diff(stocks)/lag(stocks, -1)
#head(returns_data)
#data <- cbind(diff(pAAPL), diff(pBA), diff(pJNJ), diff(pJPM), diff(pFE), diff(pDIS), diff(pVZ), diff(pPG))
#stocks <- cbind(pAAPL, pJNJ, pJPM, pFE, pDIS, pVZ, pPG)
#data <- cbind(diff(pAAPL)-1, diff(pBA)-1, diff(pJNJ)-1, diff(pJPM)-1, diff(pFE)-1, diff(pDIS)-1, diff(pVZ)-1, diff(pPG)-1)
#colnames(data)
names(data)[1] = "Apple"
names(data)[2] = "Boeing"
names(data)[3] = "Johnson & Johnson"
names(data)[4] = "JP Morgan"
names(data)[5] = "FirstElectric"
names(data)[6] = "Walt Disney"
names(data)[7] = "Verizon"
names(data)[8] = "Proctor & Gamble"
summary(data)
# Returns
mu = colMeans(data)
mu = data.frame(mu)
mu
Vmat = var(data)
Vmat
p=8
Amat=cbind(rep(1,p),mu$mu)
N=100
mup=seq(0,0.02,length=N) #the range must be within a similar range of mu1, mu2 and rf
weights=matrix(0,N,p) # populate it, N rows and p columns . contain all the weights
sdP=mup #storage vector for sd
for(i in 1:N){
bvec=c(1,mup[i]) #constraint
result=solve.QP(Dmat=2*Vmat,dvec=rep(0,p),Amat=Amat,bvec=bvec,meq=2)
# meq=2, first two constraints are strictly equality, subseqent constraints are inequalities
sdP[i]=sqrt(result$value)
weights[i,]=result$solution
}
library(tseries)
library(moments)
library(matrixStats)
library(knitr)
library(ggplot2)
library(lubridate)
library(quadprog)
p=8
Amat=cbind(rep(1,p),mu$mu)
N=100
mup=seq(0,0.02,length=N) #the range must be within a similar range of mu1, mu2 and rf
weights=matrix(0,N,p) # populate it, N rows and p columns . contain all the weights
sdP=mup #storage vector for sd
for(i in 1:N){
bvec=c(1,mup[i]) #constraint
result=solve.QP(Dmat=2*Vmat,dvec=rep(0,p),Amat=Amat,bvec=bvec,meq=2)
# meq=2, first two constraints are strictly equality, subseqent constraints are inequalities
sdP[i]=sqrt(result$value)
weights[i,]=result$solution
}
plot(x=sdP, y = mup, main = "Efficient Frontier")
min(sdP)
Amat=cbind(rep(1,p),mu$mu,diag(rep(1,p)))
N=100
mup=seq(min(mu$mu),max(mu$mu),length=N) #the range must be within a similar range of mu1, mu2 and rf
weights=matrix(0,N,p) # populate it, N rows and p columns . contain all the weights
sdP=mup #storage vector for sd
for(i in 1:N){
bvec=c(1,mup[i], rep(0,p)) #constraint
result=solve.QP(Dmat=2*Vmat,dvec=rep(0,p),Amat=Amat,bvec=bvec,meq=2)
# meq=2, first two constraints are strictly equality, subseqent constraints are inequalities
sdP[i]=sqrt(result$value)
weights[i,]=result$solution
}
plot(x=sdP, y = mup)
p=8
Amat=cbind(rep(1,p),mu$mu)
N=100
mup=seq(min(mu$mu),max(mu$mu),length=N) #the range must be within a similar range of mu1, mu2 and rf
weights=matrix(0,N,p) # populate it, N rows and p columns . contain all the weights
sdP=mup #storage vector for sd
for(i in 1:N){
bvec=c(1,mup[i]) #constraint
result=solve.QP(Dmat=2*Vmat,dvec=rep(0,p),Amat=Amat,bvec=bvec,meq=2)
# meq=2, first two constraints are strictly equality, subseqent constraints are inequalities
sdP[i]=sqrt(result$value)
weights[i,]=result$solution
}
plot(x=sdP, y = mup, main = "Efficient Frontier")
Amat=cbind(rep(1,p),mu$mu,diag(rep(1,p)))
N=100
mup=seq(min(mu$mu),max(mu$mu),length=N) #the range must be within a similar range of mu1, mu2 and rf
weights=matrix(0,N,p) # populate it, N rows and p columns . contain all the weights
sdP=mup #storage vector for sd
for(i in 1:N){
bvec=c(1,mup[i], rep(0,p)) #constraint
result=solve.QP(Dmat=2*Vmat,dvec=rep(0,p),Amat=Amat,bvec=bvec,meq=2)
# meq=2, first two constraints are strictly equality, subseqent constraints are inequalities
sdP[i]=sqrt(result$value)
weights[i,]=result$solution
}
lines(x=sdP, y = mup, col = 'red')
help(paste0)
help("colMeans")
install.packages("PerformanceAnalytics")
help(abline)
library(tseries)
library(moments)
library(tidyr)
library(tidyverse)
library(fredr)
library(pander)
library('stats')
library(openxlsx)
source('~/Desktop/MBusA 19/Module 5/Finance Analytics/Week3/In Class Examples/latent_factor.R', echo=TRUE)
source('~/Desktop/MBusA 19/Module 5/Finance Analytics/Week3/In Class Examples/latent_factor.R', echo=TRUE)
B
BB
Psi
D
library(tseries)
##############
# close any previous plots
graphics.off()
# load prices and dividend adjusted prices on BHP from Yahoo Finance
start="2001-01-01"
end="2019-01-01"
pIBM=get.hist.quote("MSFT", start=start, end=end, quote = "Adjusted",retclass="zoo",compression="m")
pIRX=get.hist.quote("^IRX", start=start, end=end, quote ="Adjusted",retclass="zoo",compression="m")
pTest=get.hist.quote("IBM", start=start, end=end, quote = "Adjusted",retclass="zoo",compression="m")
T=length(pIBM)
Rfree=pIRX[2:T]/1200   #this is the monthly risk free rate of return (simple net)
RIBM=exp(diff(log(pIBM)))-1   #this is the simple net return on IBM stock
RTest=exp(diff(log(pTest)))-1
sharpe_ratio = mean(RIBM-Rfree)/sd(RIBM-Rfree)
sharpe_ratio
knitr::opts_chunk$set(echo = TRUE)
start="1984-03-08"
pORD=get.hist.quote("^AORD", start=start, quote = "Close",retclass="zoo",compression="m")
knitr::opts_chunk$set(echo = TRUE)
library(tseries)
library(moments)
#library(matrixStats)
library(knitr)
library(ggplot2)
library(lubridate)
library(quadprog)
start="1984-03-08"
pORD=get.hist.quote("^AORD", start=start, quote = "Close",retclass="zoo",compression="m")
plot(pORD)
returns = diff(log(pORD)) # log returns
plot(returns)
mu = mean(returns[1:383])
sigma = sd(returns[1:383])
m = mu*12
lambda = sigma*sqrt(12)
# log - price
m
lambda
# price
lambda
mu_m = m + ((lambda^2)/2)
mu_m
# Question 3
m_daily = m/365
lambda_daily = lambda/365
print(m_daily)
print(lambda_daily)
# Question 4
#x0 = returns[383]
x0 = log(pORD[384])
a = m_daily
b = lambda_daily
M = 1000
N = 12/365
pAORDd=get.hist.quote("^AORD", start = '2016-07-01', end = '2016-07-01',quote = "Close",retclass="zoo",compression="d")
genGW <- function(a,b,N,T,X0){
# generalized weiner process
# a - deterministic, b - random, X0 is the value at 0
X <- rep(0,N)
W <- rep(0,N)
Dt <- T/N
tk <- seq(Dt,T,by=Dt)
sDt <- sqrt(Dt)
# first, generate RW process
epsi <- rnorm(N,0,1)
W[1] <- epsi[1]*sDt
for (t in 2:N)
{
W[t] <- W[t-1] + epsi[t]*sDt
}
# transform to GW process
X <- X0 + a*tk + b*W
return(X)
}
x0 = log(pAORD[384])
pAORDd=get.hist.quote("^AORD", start = '2016-07-01', end = '2016-07-01',quote = "Close",retclass="zoo",compression="d")
genGW <- function(a,b,N,T,X0){
# generalized weiner process
# a - deterministic, b - random, X0 is the value at 0
X <- rep(0,N)
W <- rep(0,N)
Dt <- T/N
tk <- seq(Dt,T,by=Dt)
sDt <- sqrt(Dt)
# first, generate RW process
epsi <- rnorm(N,0,1)
W[1] <- epsi[1]*sDt
for (t in 2:N)
{
W[t] <- W[t-1] + epsi[t]*sDt
}
# transform to GW process
X <- X0 + a*tk + b*W
return(X)
}
x0 = log(pAORDd[384])
pAORDd
pAORDd=get.hist.quote("^AORD", start = '2016-07-01', end = '2016-07-01',quote = "Close",retclass="zoo",compression="d")
genGW <- function(a,b,N,T,X0){
# generalized weiner process
# a - deterministic, b - random, X0 is the value at 0
X <- rep(0,N)
W <- rep(0,N)
Dt <- T/N
tk <- seq(Dt,T,by=Dt)
sDt <- sqrt(Dt)
# first, generate RW process
epsi <- rnorm(N,0,1)
W[1] <- epsi[1]*sDt
for (t in 2:N)
{
W[t] <- W[t-1] + epsi[t]*sDt
}
# transform to GW process
X <- X0 + a*tk + b*W
return(X)
}
x0 = log(pAORDd)
# using yearly results
T <- 12/365
a = m
b = lambda
N=50
Nrays <- 10000
PT <- rep(0,Nrays)
for (i in 1:Nrays){
P <- genGW(a,b,N,T,as.double(x0))
PT[i] <- exp(P[N])
}
hist(PT,75)
print(mean(PT))
# histogram of simulated price values
hist(PT, breaks = 75,
main = "Histogram of simulated prices after 13 days",
xlab = "Simulated price")
x0
a
b
pAORDd
a
b
# Question 5
# Expected value of the Index
mean(PT)
# Variance of the index
var(PT)
quantile(PT, probs = c(.01, .99))
#e
mean(PT[PT<VaR1])
VaR1 = quantile(PT, probs = .01)
VaR99 = quantile(PT, probs = .99)
print(VaR1)
print(VaR99)
#e
mean(PT[PT<VaR1])
#f
mean(PT[PT>VaR99])
lambda*sqrt(365)
mu = mean(returns[1:383])
sigma = sd(returns[1:383])
m = mu*12
lambda = sigma*sqrt(12)
# log - price
m
lambda
# price
lambda
mu_m = m + ((lambda^2)/2)
mu_m
# Question 3
m_daily = m/365
lambda_daily = lambda/365
print(m_daily)
print(lambda_daily)
lambda*(1/sqrt(365))
m/365
lambda*(1/sqrt(365))
m
pAORDd=get.hist.quote("^AORD", start = '2016-07-01', end = '2016-07-01',quote = "Close",retclass="zoo",compression="d")
genGW <- function(a,b,N,T,X0){
# generalized weiner process
# a - deterministic, b - random, X0 is the value at 0
X <- rep(0,N)
W <- rep(0,N)
Dt <- T/N
tk <- seq(Dt,T,by=Dt)
sDt <- sqrt(Dt)
# first, generate RW process
epsi <- rnorm(N,0,1)
W[1] <- epsi[1]*sDt
for (t in 2:N)
{
W[t] <- W[t-1] + epsi[t]*sDt
}
# transform to GW process
X <- X0 + a*tk + b*W
return(X)
}
x0 = log(pAORDd)
# using yearly results
T <- 12/365
a = m
b = lambda
N=50
Nrays <- 10000
PT <- rep(0,Nrays)
for (i in 1:Nrays){
P <- genGW(a,b,N,T,as.double(x0))
PT[i] <- exp(P[N])
}
hist(PT,75)
print(mean(PT))
install.packages("mlogit")
setwd("~/Documents/GitHub/engineers.without.stats/Case Study/dataFiles/model")
setwd("~/Documents/GitHub/engineers.without.stats/Case Study/dataFiles/model")
library(MASS)
library(foreign)
library(mlogit)
business = read.csv('dataset.csv')
head(business)
#The ordered logit model
fit_olog1=polr(stars~review_count+RestaurantsPriceRange2+RestaurantsReservations,data=business,method="logistic")
library(MASS)
library(foreign)
library(mlogit)
library(magrittr)
business$stars %<>% factor
business$RestaurantsPriceRange2 %<>% factor
business$RestaurantsReservations %<>% factor
#The ordered logit model
fit_olog1=polr(stars~review_count+RestaurantsPriceRange2+RestaurantsReservations,data=business,method="logistic")
summary(fit_olog1)
#A function to produce p-values from a polr fit
polr_pval<-function(fit){
fit.c=coef(summary(fit))
fit.c=cbind(fit.c,"p-val"=pnorm(abs(fit.c[,"t value"]),lower.tail=FALSE)*2)
return(fit.c)
}
#computing the p-values of each coefficient
fit_olog1.c=polr_pval(fit_olog1)
summary(fit_olog1)
#computing the p-values of each coefficient
fit_olog1.c=polr_pval(fit_olog1)
fit_olog1.c
business$RestaurantsTakeOut %<>% factor
business$RestaurantsGoodForGroups %<>% factor
business$GoodForKids %<>% factor
business$RestaurantsDelivery %<>% factor
business$RestaurantsAttire %<>% factor
business$WiFi %<>% factor
fit_olog2=polr(stars~review_count+RestaurantsPriceRange2+RestaurantsReservations+RestaurantsTakeOut+RestaurantsGoodForGroups+GoodForKids+RestaurantsDelivery+RestaurantsAttire+WiFi,data=business,method="logistic")
summary(fit_olog2)
business = read.csv('dataset_2.csv')
head(business)
business$stars %<>% factor
business$RestaurantsPriceRange2 %<>% factor
business$RestaurantsReservations %<>% factor
business$RestaurantsTakeOut %<>% factor
business$RestaurantsGoodForGroups %<>% factor
business$GoodForKids %<>% factor
business$RestaurantsDelivery %<>% factor
business$RestaurantsAttire %<>% factor
business$WiFi %<>% factor
business$casual %<>% factor
business$classy %<>% factor
business$divey %<>% factor
business$hipster %<>% factor
business$intimate %<>% factor
business$romantic %<>% factor
business$touristy %<>% factor
business$trendy %<>% factor
business$upscale %<>% factor
business$Alcohol %<>% factor
View(business)
fit_olog3=polr(stars~review_count+RestaurantsPriceRange2+RestaurantsReservations+RestaurantsTakeOut+RestaurantsGoodForGroups+GoodForKids+RestaurantsDelivery+RestaurantsAttire+WiFi+casual+classy+divey+hipster+intimate+romantic+touristy+trendy+upscale+Alcohol,data=business,method="logistic")
summary(fit_olog3)
fit_olog3.c=polr_pval(fit_olog3)
fit_olog3.c
